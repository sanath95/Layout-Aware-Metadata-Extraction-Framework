# Layout-Aware Metadata Extraction Framework

## Table of contents
1. [Environment Setup](#environment-setup)
2. [Notebook Collection](#notebook-collection)
    1. [Metadata Extraction](#metadata-extraction)
    2. [PDF Extraction](#pdf-extraction)

---

## Environment Setup

**Data**
All the data can be found in this [Shared Drive](https://drive.google.com/drive/folders/1JLieCvkfdLg80YOuZHsK2NEBlJtovGY6?usp=sharing).

The data folder structure looks like this:
```
Layout Aware Metadata Extraction Data
└───metadata_extraction_data     # datasets and outputs used for metadata extraction
│   │   gpt_oss_metadata
│   │   grobid_crf_metadata
│   │   grobid_crf_tei           # TEI outputs from GROBID CRF model
│   │   grobid_dl_metadata
│   │   grobid_dl_tei            # TEI outputs from GROBID DL model
│   │   llama3b_metadata
│   │   metadata                 # ground-truth metadata annotations
│   │   pdf                      # selected PDF files used for metadata extraction
│   │   phi4mini_metadata
│   │   qwen3b_metadata
│   │   qwen4b_metadata
└───pdf_extraction_data          # datasets and outputs used for PDF text extraction
    │   annotation               # annotations derived from DocBank dataset
    │   img                      # layout segmentation results from YOLOv12-DocLayNet
    │   pdf                      # 101 selected PDF files for parser evaluation
    │   pdfalto_xml              # XML outputs generated by pdfalto
    │   txt                      # ground-truth plain text for comparison
    │   arxiv_api_response.json  # API response from arXiv containing metadata
    │   metadata.csv             # mapping file linking input PDFs to output paths
    
```

**Python environment**
Create and activate a virtual environment, then install dependencies declared in `pyproject.toml`.
```bash
uv venv --python 3.11
source .venv/bin/activate
uv pip install --upgrade pip
uv sync
```
> The `pyproject.toml` includes optional CUDA wheels for PyTorch via the `pytorch-cu128` index; install them on a GPU-enabled machine for the layout and LLM notebooks.

**LayoutReader**
Required in creating ground truth data to evaluate PDF parsers.
```bash
git clone https://github.com/ppaanngggg/layoutreader.git
```

**Services**
1. Run [GROBID with Docker container](https://grobid.readthedocs.io/en/latest/Grobid-docker/).
2. Install and start [Ollama](https://ollama.com/) to execute the [GPT-OSS](https://ollama.com/library/gpt-oss) model.
3. Use [pdfalto](https://github.com/kermitt2/pdfalto) to produce XML representations of the PDFs.
4. `pdf2image` relies on native library (`poppler`). Required in creating ground truth data to evaluate PDF parsers.

**Environment variables**
The metadata notebooks expect an `.env` file or shell environment providing `OPENAI_API_KEY` (used as an OpenRouter key) and `HF_TOKEN` (for meta-llama/Llama-3.2-3B-Instruct model). Place the `.env` file next to the notebook or export the variables before launching Jupyter.

---

## Notebook Collection

**Folder Structure**
```
Notebooks
└───metadata_extraction
│   │   config.yaml                                # prompts, target metadata fields, extraction and evaluation models
│   │   grobid_config.json                         # connection details for GROBID server
│   │   create_metadata_ground_truth.ipynb
│   │   grobid_extract_metadata.ipynb
│   │   language_models_extract_metadata.ipynb
│   │   eval_metadata_extraction.ipynb
└───pdf_extraction
│   │   arxiv_docbank_pipeline.ipynb
│   │   data_prep_pipeline.ipynb
│   │   eval_pdf_text_extraction.ipynb
```

---

### Metadata Extraction
(`Notebooks/metadata_extraction/`)

* **create_metadata_ground_truth.ipynb**
This notebook addresses the challenge of constructing a reliable ground‑truth metadata dataset. Given that individual LLMs may hallucinate or disagree on specific fields, the approach is to extract the full text of each PDF and query several LLMs with identical prompts. Their responses are parsed into structured JSON, compared field‑by‑field and, when all models agree, the value is accepted. When there is disagreement, a dedicated judge model is called to arbitrate. The result is a consensus metadata record that is written to disk for use as ground truth.

<p align="center">
<img src="./assets/create_metadata_ground_truth.png" alt="create metadata ground truth" width="50%"/>
</p>

* **grobid_extract_metadata.ipynb**
This notebook provides a classical, rule‑based baseline for metadata extraction. The problem is to extract bibliographic fields from PDFs in a deterministic and reproducible manner. The approach is to use the open‑source GROBID tool on each PDF to produce structured TEI XML, then parse the XML into normalised JSON. This baseline serves both as a comparison point for LLM methods and as an additional source of metadata that could be used in ensemble systems. calls the GROBID service to produce TEI files and then converts them into simplified JSON metadata and stored for downstream use.

<p align="center">
<img src="./assets/grobid_extract_metadata.png" alt="grobid extract metadata" width="50%"/>
</p>

* **language_models_extract_metadata.ipynb**
The goal of this notebook is to benchmark individual language models as standalone metadata extractors. Given a PDF, it is unclear how well an off‑the‑shelf LLM can recover the title, authors, affiliations and other fields. The approach is to load a chosen HuggingFace model, extract the first page of each document, craft a system prompt describing the desired JSON schema and generate a response. The notebook then parses the model’s output, validates it against a pydantic schema and writes the resulting metadata to disk along with timing information. This notebook also consists of code to run inference using the `gpt-oss` model on Ollama.

<p align="center">
<img src="./assets/language_models_extract_metadata.png" alt="language models extract metadata" width="50%"/>
</p>
 
* **eval_metadata_extraction.ipynb**
After creating ground truth and individual predictions, the next problem is to quantitatively assess which extractors perform best on which fields. This notebook loads the ground truth and the outputs of GROBID and the various LLMs, normalises the text and computes a suite of similarity metrics (Levenshtein distance, cosine similarity, F1‑score). It aggregates these metrics into tables and plots so the reader can identify strengths and weaknesses across systems. In doing so it ties the theoretical metrics discussed in the thesis to practical results.

<p align="center">
<img src="./assets/eval_metadata_extraction.png" alt="eval metadata extraction" width="50%"/>
</p>

### PDF extraction
(`Notebooks/pdf_extraction/`)

* **arxiv_docbank_pipeline.ipynb**
The first step in building the text‑extraction benchmark is assembling a dataset of PDFs paired with token‑level annotations. The problem is twofold: to select a diverse set of papers from arXiv (across years 2014-2018 and subject categories like CS, economics, math, etc.) and to find matching DocBank annotations. The notebook solves this by querying the arXiv API according to user‑defined categories and years, parsing the feed for article identifiers, matching those identifiers to DocBank’s annotation files via regular expressions, downloading both PDFs and annotations and saving the results in a CSV file. This curated metadata file forms the foundation for subsequent notebooks.

<p align="center">
<img src="./assets/arxiv_docbank_pipeline.png" alt="arxiv docbank pipeline" width="50%"/>
</p>

* **data_prep_pipeline.ipynb**
Once PDFs and annotations are in place, the challenge is to recover the correct reading order of tokens on each page. this notebook implements a two‑stage approach inspired by the thesis: first run a YOLO object detector to locate layout regions, then group DocBank tokens into these regions and employ LayoutLMv3 to predict their sequence. The pipeline outputs a plain‑text file per page with tokens ordered as a human would read them, along with an annotated image showing detected regions.

<p align="center">
<img src="./assets/data_prep_pipeline.png" alt="data prep pipeline" width="50%"/>
</p>

* **eval_pdf_text_extraction.ipynb**
With ground‑truth reading order in hand, the final step is to benchmark existing PDF parsers and quantify their shortcomings. This notebook extracts text from each PDF using five different libraries (PyMuPDF, pypdfium2, pdfminer, PyPDF2 and pdfalto), cleans the outputs and compares them to the ground truth using character‑ and word‑level error rates, semantic similarity metrics (BLEU/ROUGE) and an order error based on Kendall τ. The scores are aggregated and visualised via box plots, providing a clear empirical narrative of the observable performance differences among parsers.

<p align="center">
<img src="./assets/eval_pdf_text_extraction.png" alt="eval pdf text extraction" width="50%"/>
</p>

---
---