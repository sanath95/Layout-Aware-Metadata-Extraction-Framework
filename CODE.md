# Layout-Aware Metadata Extraction Framework

## Table of contents
1. [Notebook Collection](#notebook-collection)
2. [Environment Setup](#environment-setup)

## Notebook Collection

Notebooks
```
└───metadata_extraction
│   │   config.yaml
│   │   grobid_config.json
│   │   create_metadata_ground_truth.ipynb
│   │   grobid_extract_metadata.ipynb
│   │   language_models_extract_metadata.ipynb
│   │   eval_metadata_extraction.ipynb
└───pdf_extraction
│   │   arxiv_docbank_pipeline.ipynb
│   │   data_prep_pipeline.ipynb
│   │   eval_pdf_text_extraction.ipynb
```

---

### Metadata extraction (`Notebooks/metadata_extraction/`)
- `config.yaml` centralizes the LLM system prompts, target metadata fields, and the list of extraction and evaluation models so you can swap APIs without editing notebook cells. Update the values to reflect the models and prompts available in your account before running the LLM workflows.
- `grobid_config.json` stores the connection details for a running GROBID server (host, batch size, timeouts, etc.). Adjust it to point at your deployment before executing the GROBID notebook.
- `create_metadata_ground_truth.ipynb` orchestrates an ensemble of LLM extractors. It reads PDF text, queries each model defined in `config.yaml`, reconciles disagreements at the field level, and records adjudicated JSON metadata under `data/metadata_extraction_data/metadata/`.
- `language_models_extract_metadata.ipynb` demonstrates running local or hosted instruction-tuned models (e.g., Phi-4-mini, Qwen, GPT-OSS via Ollama) to produce structured metadata JSON. It includes utilities for paginated PDF text extraction, enforcing a Pydantic schema, and saving per-paper results to directories such as `phi4mini_demo_metadata/`.
- `grobid_extract_metadata.ipynb` calls the GROBID service to produce TEI files and then converts them into simplified JSON metadata dumps stored in `data/metadata_extraction_data/grobid_*_metadata/`.
- `eval_metadata_extraction.ipynb` scores multiple metadata hypotheses against the ground-truth set using string cleaning, Levenshtein distance, precision/recall/F1, and aggregation utilities that summarize performance across models.

### PDF extraction (`Notebooks/pdf_extraction/`)
- `arxiv_docbank_pipeline.ipynb` seeds the corpus by querying the arXiv API, downloading PDFs, and pairing them with DocBank annotations fetched from the Hugging Face Hub. It assembles a `metadata.csv`, cached API responses, and aligned text/annotation assets under `data/pdf_extraction_data/`.
- `data_prep_pipeline.ipynb` prepares training/evaluation inputs by running document layout detection (YOLO DocLayNet weights) and LayoutLMv3 token classifiers. It aligns detected regions with DocBank tokens, renders supporting page images, and fills missing text entries in `metadata.csv`.
- `eval_pdf_text_extraction.ipynb` benchmarks popular PDF text extraction libraries (PyMuPDF, pypdfium2, pdfminer, PyPDF2, PDFAlto) against the DocBank ground truth, reporting WER/CER, BLEU/ROUGE, rank correlations, and summary tables saved to `data/outputs/`.

## Environment Setup

### Data
All the data can be found in this [Shared Drive](https://drive.google.com/drive/folders/1JLieCvkfdLg80YOuZHsK2NEBlJtovGY6?usp=sharing).

The data folder structure looks like this:
```
Layout Aware Metadata Extraction Data
└───metadata_extraction_data     # datasets and outputs used for metadata extraction
│   │   gpt_oss_metadata
│   │   grobid_crf_metadata
│   │   grobid_crf_tei           # TEI outputs from GROBID CRF model
│   │   grobid_dl_metadata
│   │   grobid_dl_tei            # TEI outputs from GROBID DL model
│   │   llama3b_metadata
│   │   metadata                 # ground-truth metadata annotations
│   │   pdf                      # selected PDF files used for metadata extraction
│   │   phi4mini_metadata
│   │   qwen3b_metadata
│   │   qwen4b_metadata
└───pdf_extraction_data          # datasets and outputs used for PDF text extraction
    │   annotation               # annotations derived from DocBank dataset
    │   img                      # layout segmentation results from YOLOv12-DocLayNet
    │   pdf                      # 101 selected PDF files for parser evaluation
    │   pdfalto_xml              # XML outputs generated by pdfalto
    │   txt                      # ground-truth plain text for comparison
    │   arxiv_api_response.json  # API response from arXiv containing metadata
    │   metadata.csv             # mapping file linking input PDFs to output paths
    
```

### Python environment
Create and activate a virtual environment, then install dependencies declared in `pyproject.toml`.
```bash
uv venv --python 3.11
source .venv/bin/activate
uv pip install --upgrade pip
uv sync
```
> The `pyproject.toml` includes optional CUDA wheels for PyTorch via the `pytorch-cu128` index; install them on a GPU-enabled machine for the layout and LLM notebooks.

### LayoutReader
Required in creating ground truth data to evaluate PDF parsers.
```bash
git clone https://github.com/ppaanngggg/layoutreader.git
```

### Services
1. Run [GROBID with Docker container](https://grobid.readthedocs.io/en/latest/Grobid-docker/).
2. Install and start [Ollama](https://ollama.com/) to execute the [GPT-OSS](https://ollama.com/library/gpt-oss) model.
3. Use [pdfalto](https://github.com/kermitt2/pdfalto) to produce XML representations of the PDFs.
4. `pdf2image` relies on native library (`poppler`). Required in creating ground truth data to evaluate PDF parsers.

### Environment variables
The metadata notebooks expect an `.env` file or shell environment providing `OPENAI_API_KEY` (used as an OpenRouter key) and `HF_TOKEN` (for meta-llama/Llama-3.2-3B-Instruct model). Place the `.env` file next to the notebook or export the variables before launching Jupyter.