{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e739f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "from typing import List, Dict\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from strsimpy.levenshtein import Levenshtein\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb180d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_dir = \"../../data/metadata_extraction_data/metadata\"\n",
    "pred_dir = \"../../data/metadata_extraction_data/grobid_metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_controls(text: str) -> str:\n",
    "    \"\"\"Drop all Unicode control characters (category C*).\"\"\"\n",
    "    return \"\".join(c for c in text if unicodedata.category(c)[0] != \"C\")\n",
    "\n",
    "def _fix_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove diacritics (accents) but keep other characters unchanged.\n",
    "    \"\"\"\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def _clean_item(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lower-case, strip accents, trim whitespace, collapse internal spaces.\n",
    "    \"\"\"\n",
    "    text = _strip_controls(_fix_accents(text.lower()))\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def levenshtein_distance(s0, s1):\n",
    "    return Levenshtein().distance(s0, s1)\n",
    "\n",
    "def cosine_sim(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    TF-IDF cosine similarity between two texts.\n",
    "    \"\"\"\n",
    "    if a and b:\n",
    "        vect = TfidfVectorizer(stop_words=\"english\")\n",
    "        mat = vect.fit_transform([a, b])\n",
    "        return round(float(cosine_similarity(mat[0:1], mat[1:2])[0, 0]), 2)\n",
    "    else: return 1\n",
    "\n",
    "def f1_on_sets(true_set, pred_set):\n",
    "    \"\"\"\n",
    "    Element-level F1 score treating both inputs as sets after normalisation.\n",
    "    \"\"\"\n",
    "    if not true_set and not pred_set:\n",
    "        return 1, 1, 1\n",
    "    if true_set and not pred_set:\n",
    "        return 0, 0, 0\n",
    "    if not true_set and pred_set:\n",
    "        return 0, 0, 0\n",
    "    if len(true_set) == 1 and len(pred_set) == 1:\n",
    "        return float(true_set == pred_set), float(true_set == pred_set), float(true_set == pred_set)\n",
    "    labels = sorted(true_set|pred_set)                 # union of both sets\n",
    "\n",
    "    # ── 3. Convert each set to a binary indicator row (1 = label present) ─────────\n",
    "    mlb = MultiLabelBinarizer(classes=labels)\n",
    "    y_true_bin = mlb.fit_transform([true_set])   # shape (1, n_labels)\n",
    "    y_pred_bin = mlb.transform([pred_set])  \n",
    "    \n",
    "    # ── 4. Micro-averaged metrics for multi-label classification ───────────────────\n",
    "    prec  = round(precision_score(y_true_bin, y_pred_bin, average='samples'), 2)\n",
    "    rec   = round(recall_score(y_true_bin, y_pred_bin, average='samples'), 2)\n",
    "    f1    = round(f1_score(y_true_bin, y_pred_bin, average='samples'), 2)\n",
    "\n",
    "    return (prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeacd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_record(\n",
    "    y_true: Dict, y_predicted: Dict\n",
    ") -> List[Dict[str, object]]:\n",
    "    rows: List[Dict[str, object]] = []\n",
    "\n",
    "    for field in (\"title\", \"doi\", \"publication_date\", \"publisher\"):\n",
    "        gt = _clean_item(y_true.get(field, \"\"))\n",
    "        pd_ = _clean_item(y_predicted.get(field, \"\"))\n",
    "        match = levenshtein_distance(gt, pd_)\n",
    "        rows.append(\n",
    "            dict(\n",
    "                field=field,\n",
    "                metric=\"levenshtein_distance\",\n",
    "                score=float(match),\n",
    "                ground_truth=gt,\n",
    "                predicted=pd_,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    for field in [\"abstract\"]:\n",
    "        gt = _clean_item(y_true.get(field, \"\"))\n",
    "        pd_ = _clean_item(y_predicted.get(field, \"\"))\n",
    "        cos_similarity = cosine_sim(gt, pd_)\n",
    "        rows.append(\n",
    "            dict(\n",
    "                field=field,\n",
    "                metric=\"cosine_similarity\",\n",
    "                score=cos_similarity,\n",
    "                ground_truth=gt,\n",
    "                predicted=pd_,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for field in (\"authors\", \"affiliations\", \"keywords\"):\n",
    "        gt_list = y_true.get(field, [])\n",
    "        pd_list = y_predicted.get(field, [])\n",
    "        true_set = {_clean_item(x) for x in gt_list}\n",
    "        pred_set = {_clean_item(x) for x in pd_list}\n",
    "        _,_,f1 = f1_on_sets(true_set, pred_set)\n",
    "        rows.append(\n",
    "            dict(\n",
    "                field=field,\n",
    "                metric=\"F1_set\",\n",
    "                score=f1,\n",
    "                ground_truth=\"; \".join(true_set),\n",
    "                predicted=\"; \".join(pred_set),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(str(Path(ground_truth_dir)/\"*.json\"))\n",
    "files.remove('..\\\\..\\\\data\\\\metadata_extraction_data\\\\metadata\\\\appaw-et-al-leveraging-advances-in-machine-learning-for-the-robust-classification-and-interpretation-of-networks.json')\n",
    "all_metrics=[]\n",
    "for ground_truth_file in files:\n",
    "    name = Path(ground_truth_file).stem\n",
    "    pred_file = Path(pred_dir) / f\"{name}.grobid.json\"\n",
    "    with open(pred_file, encoding=\"utf-8\") as f:\n",
    "        y_pred = load(f)\n",
    "\n",
    "    with open(ground_truth_file, encoding=\"utf-8\") as f:\n",
    "        y = load(f)\n",
    "    results = evaluate_record(y, y_pred)\n",
    "    all_metrics.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "scores_by_field = {}\n",
    "for record in all_metrics:\n",
    "    for entry in record:\n",
    "        field = entry['field']\n",
    "        score = entry['score']\n",
    "        scores_by_field.setdefault(field, []).append(score)\n",
    "\n",
    "# --- Compute the average for each field --------------------------------------\n",
    "avg_scores = {field: mean(vals) for field, vals in scores_by_field.items()}\n",
    "\n",
    "# --- Present as a DataFrame for clarity --------------------------------------\n",
    "avg_df = pd.DataFrame({\n",
    "    'field': list(avg_scores.keys()),\n",
    "    'average_score': list(avg_scores.values())\n",
    "}).sort_values('field').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb747da",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = [item for sublist in all_metrics for item in sublist]\n",
    "df = pd.DataFrame(flat_data)\n",
    "\n",
    "# Define field groups\n",
    "group1_fields = ['title', 'doi', 'publication_date', 'publisher']\n",
    "group2_fields = ['abstract']\n",
    "group3_fields = ['authors', 'affiliations', 'keywords']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=False)\n",
    "\n",
    "# Plot each group with dynamic y-limits\n",
    "for ax, fields, title, y_field in zip(\n",
    "    axes,\n",
    "    [group1_fields, group2_fields, group3_fields],\n",
    "    ['Levenshtein Distance', 'Cosine Similarity', 'F1 Score'],\n",
    "    [\"Levenshtein Distance\", \"Cosine Similarity\", \"F1 Score\"]\n",
    "):\n",
    "    subset = df[df['field'].isin(fields)]\n",
    "    sns.boxplot(data=subset, x='field', y='score', hue='metric', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Field')\n",
    "    ax.set_ylabel(y_field)\n",
    "    ax.legend(loc=False)\n",
    "\n",
    "    # Dynamic y-axis limits\n",
    "    ymin = subset['score'].min()\n",
    "    ymax = subset['score'].max()\n",
    "    padding = (ymax - ymin) * 0.1 if ymax != ymin else 0.1\n",
    "    ax.set_ylim(ymin - padding, ymax + padding)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = [item for sublist in all_metrics for item in sublist]\n",
    "df = pd.DataFrame(flat_data)\n",
    "\n",
    "# Filter for abstract field\n",
    "abstract_df = df[df['field'] == 'abstract']\n",
    "\n",
    "# Create KDE plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=abstract_df, x='score', hue='metric', fill=True, common_norm=False, palette='crest', alpha=0.6)\n",
    "plt.title('KDE Plot of Abstract Cosine Similarity')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd0d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
