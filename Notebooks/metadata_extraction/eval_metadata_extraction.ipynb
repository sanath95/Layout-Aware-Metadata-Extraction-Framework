{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "from typing import List, Dict\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from strsimpy.levenshtein import Levenshtein\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_dir = \"../../data/metadata_extraction_data/metadata\"\n",
    "\n",
    "hyps = {\n",
    "    \"grobid_dl\": \"../../data/metadata_extraction_data/grobid_dl_metadata\",\n",
    "    \"grobid_crf\": \"../../data/metadata_extraction_data/grobid_crf_metadata\",\n",
    "    \"gpt_oss\": \"../../data/metadata_extraction_data/gpt_oss_metadata\",\n",
    "    \"phi4_mini\": \"../../data/metadata_extraction_data/phi4mini_metadata\",\n",
    "    \"qwen3b\": \"../../data/metadata_extraction_data/qwen3b_metadata\",\n",
    "    \"qwen4b\": \"../../data/metadata_extraction_data/qwen4b_metadata\",\n",
    "    \"llama3b\": \"../../data/metadata_extraction_data/llama3b_metadata\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_controls(text: str) -> str:\n",
    "    \"\"\"Drop all Unicode control characters (category C*).\"\"\"\n",
    "    return \"\".join(c for c in text if unicodedata.category(c)[0] != \"C\")\n",
    "\n",
    "def _fix_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove diacritics (accents) but keep other characters unchanged.\n",
    "    \"\"\"\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def _clean_item(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lower-case, strip accents, trim whitespace, collapse internal spaces.\n",
    "    \"\"\"\n",
    "    text = _strip_controls(_fix_accents(text.lower()))\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def levenshtein_distance(s0, s1):\n",
    "    return Levenshtein().distance(s0, s1)\n",
    "\n",
    "def cosine_sim(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    TF-IDF cosine similarity between two texts.\n",
    "    \"\"\"\n",
    "    if a and b:\n",
    "        vect = TfidfVectorizer(stop_words=\"english\")\n",
    "        mat = vect.fit_transform([a, b])\n",
    "        return round(float(cosine_similarity(mat[0:1], mat[1:2])[0, 0]), 2)\n",
    "    else: return 1\n",
    "\n",
    "def f1_on_sets(true_set, pred_set):\n",
    "    \"\"\"\n",
    "    Element-level F1 score treating both inputs as sets after normalisation.\n",
    "    \"\"\"\n",
    "    if not true_set and not pred_set:\n",
    "        return 1, 1, 1\n",
    "    if true_set and not pred_set:\n",
    "        return 0, 0, 0\n",
    "    if not true_set and pred_set:\n",
    "        return 0, 0, 0\n",
    "    if len(true_set) == 1 and len(pred_set) == 1:\n",
    "        return float(true_set == pred_set), float(true_set == pred_set), float(true_set == pred_set)\n",
    "    labels = sorted(true_set|pred_set)                 # union of both sets\n",
    "\n",
    "    # ── 3. Convert each set to a binary indicator row (1 = label present) ─────────\n",
    "    mlb = MultiLabelBinarizer(classes=labels)\n",
    "    y_true_bin = mlb.fit_transform([true_set])   # shape (1, n_labels)\n",
    "    y_pred_bin = mlb.transform([pred_set])  \n",
    "    \n",
    "    # ── 4. Micro-averaged metrics for multi-label classification ───────────────────\n",
    "    prec  = round(precision_score(y_true_bin, y_pred_bin, average='samples'), 2)\n",
    "    rec   = round(recall_score(y_true_bin, y_pred_bin, average='samples'), 2)\n",
    "    f1    = round(f1_score(y_true_bin, y_pred_bin, average='samples'), 2)\n",
    "\n",
    "    return (prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_record(\n",
    "    y_true: Dict, y_predicted: Dict, is_grobid: bool = False\n",
    ") -> List[Dict[str, object]]:\n",
    "    rows: List[Dict[str, object]] = []\n",
    "\n",
    "    for field in (\"title\", \"doi\", \"publication_date\", \"publisher\"):\n",
    "        gt = _clean_item(y_true.get(field, \"\"))\n",
    "        if y_predicted.get(field, \"\"): pd_ = _clean_item(y_predicted.get(field, \"\"))\n",
    "        else: pd_ = \"\"\n",
    "        match = levenshtein_distance(gt, pd_)\n",
    "        rows.append(\n",
    "            dict(\n",
    "                field=field,\n",
    "                metric=\"levenshtein_distance\",\n",
    "                score=float(match),\n",
    "                ground_truth=gt,\n",
    "                predicted=pd_,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    for field in [\"abstract\"]:\n",
    "        gt = _clean_item(y_true.get(field, \"\"))\n",
    "        pd_ = _clean_item(y_predicted.get(field, \"\"))\n",
    "        cos_similarity = cosine_sim(gt, pd_)\n",
    "        rows.append(\n",
    "            dict(\n",
    "                field=field,\n",
    "                metric=\"cosine_similarity\",\n",
    "                score=cos_similarity,\n",
    "                ground_truth=gt,\n",
    "                predicted=pd_,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for field in (\"authors\", \"affiliations\", \"keywords\", \"email_ids\"):\n",
    "        gt_list = y_true.get(field, [])\n",
    "        pd_list = y_predicted.get(field, [])\n",
    "        true_set = {_clean_item(x) for x in gt_list}\n",
    "        pred_set = {_clean_item(x) for x in pd_list}\n",
    "        true_set = {s for true_s in true_set for s in true_s.split() if s}\n",
    "        pred_set = {s for pred_s in pred_set for s in pred_s.split() if s}\n",
    "        _,_,f1 = f1_on_sets(true_set, pred_set)\n",
    "        if is_grobid and field==\"email_ids\":\n",
    "            rows.append(\n",
    "                dict(\n",
    "                    field=field,\n",
    "                    metric=\"F1_set\",\n",
    "                    score=0,\n",
    "                    ground_truth=\"; \".join(true_set),\n",
    "                    predicted=\"; \".join(pred_set),\n",
    "                )\n",
    "            )\n",
    "            return rows\n",
    "        rows.append(\n",
    "            dict(\n",
    "                field=field,\n",
    "                metric=\"F1_set\",\n",
    "                score=f1,\n",
    "                ground_truth=\"; \".join(true_set),\n",
    "                predicted=\"; \".join(pred_set),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(str(Path(ground_truth_dir)/\"*.json\"))\n",
    "all_models = {}\n",
    "for model, pred_dir in hyps.items():\n",
    "    all_metrics=[]\n",
    "    for ground_truth_file in files:\n",
    "        name = Path(ground_truth_file).stem\n",
    "        pred_file = Path(pred_dir) / f\"{name}.json\"\n",
    "        with open(pred_file, encoding=\"utf-8\") as f:\n",
    "            y_pred = load(f)\n",
    "\n",
    "        with open(ground_truth_file, encoding=\"utf-8\") as f:\n",
    "            y = load(f)\n",
    "        is_grobid = model.startswith(\"grobid\")\n",
    "        results = evaluate_record(y, y_pred, is_grobid)\n",
    "        all_metrics.append(results)\n",
    "    all_models[model] = all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "big_df = pd.DataFrame(columns=[\"fields\", \"metric\", \"grobid_dl\", \"grobid_crf\", \"gpt_oss\", \"phi4_mini\", \"qwen3b\", \"qwen4b\", \"llama3b\"])\n",
    "big_df[\"fields\"] = [\"title\", \"doi\", \"publication_date\", \"publisher\", \"abstract\", \"authors\", \"affiliations\", \"keywords\", \"email_ids\"]\n",
    "big_df[\"metric\"] = [\"Levenshtein Dist\", \"Levenshtein Dist\", \"Levenshtein Dist\",  \"Levenshtein Dist\", \"Cosine Sim\", \"F1 Score\", \"F1 Score\", \"F1 Score\", \"F1 Score\"]\n",
    "for model, all_metrics in all_models.items():\n",
    "    scores_by_field = {}\n",
    "    for record in all_metrics:\n",
    "        for entry in record:\n",
    "            field = entry['field']\n",
    "            score = entry['score']\n",
    "            scores_by_field.setdefault(field, []).append(score)\n",
    "\n",
    "    # --- Compute the average for each field --------------------------------------\n",
    "    avg_scores = {field: mean(vals) for field, vals in scores_by_field.items()}\n",
    "\n",
    "    for field in big_df[\"fields\"]:\n",
    "        big_df.loc[big_df[\"fields\"]==field, model] = avg_scores.get(field, 0)\n",
    "    # --- Present as a DataFrame for clarity --------------------------------------\n",
    "    avg_df = pd.DataFrame({\n",
    "        'field': list(avg_scores.keys()),\n",
    "        'average_score': list(avg_scores.values())\n",
    "    }).sort_values('field').reset_index(drop=True)\n",
    "    #print(avg_df.to_markdown())\n",
    "print(big_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_again(results):\n",
    "    # Define field groups\n",
    "    group1_fields = [\"title\", \"doi\", \"publication_date\", \"publisher\"]\n",
    "    group2_fields = [\"abstract\"]\n",
    "    group3_fields = [\"authors\", \"affiliations\", \"keywords\"]\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for model_name, model_data in results.items():\n",
    "        for entry in model_data:\n",
    "            for item in entry:\n",
    "                data.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"field\": item[\"field\"],\n",
    "                    \"score\": item[\"score\"]\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Box plot for group1_fields\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=\"field\", y=\"score\", hue=\"model\", data=df[df[\"field\"].isin(group1_fields)])\n",
    "    plt.title(\"Distribution of Levenshtein Distance for different Models\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"Levenshtein Distance\")\n",
    "    plt.ylim(0, 50)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../../data/outputs/group1_boxplot.png\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # KDE plot for group2_fields\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for model_name in results.keys():\n",
    "        subset = df[(df[\"model\"] == model_name) & (df[\"field\"].isin(group2_fields))]\n",
    "        sns.kdeplot(subset[\"score\"], label=model_name, fill=True)\n",
    "    plt.title(\"Distribution of Cosine Similarity for different Models\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Cosine Similarity of Abstract\")\n",
    "    plt.savefig(\"../../data/outputs/group2_boxplot.png\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Box plot for group3_fields\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=\"field\", y=\"score\", hue=\"model\", data=df[df[\"field\"].isin(group3_fields)])\n",
    "    plt.title(\"Distribution of F1 Score for different Models\")\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.savefig(\"../../data/outputs/group3_boxplot.png\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Plots generated:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_models[\"grobid_crf\"]\n",
    "del all_models[\"qwen4b\"]\n",
    "plot_again(all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"grobid_dl\", \"gpt_oss\", \"phi4_mini\", \"qwen3b\", \"llama3b\"]\n",
    "\n",
    "df = big_df\n",
    "\n",
    "def plot_cosine_and_f1(df):\n",
    "    models = [\"grobid_dl\", \"gpt_oss\", \"phi4_mini\", \"qwen3b\", \"llama3b\"]\n",
    "\n",
    "    # Keep only F1 Score and Cosine Sim metrics\n",
    "    subset = df[df[\"metric\"].isin([\"F1 Score\", \"Cosine Sim\"])].melt(\n",
    "        id_vars=[\"fields\", \"metric\"], \n",
    "        value_vars=models, \n",
    "        var_name=\"model\", \n",
    "        value_name=\"score\"\n",
    "    )\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(\n",
    "        data=subset,\n",
    "        x=\"fields\",\n",
    "        y=\"score\",\n",
    "        hue=\"model\"\n",
    "    )\n",
    "    ax.set_title(\"Average Score (Cosine Similarity and F1 Score) for different Models\")\n",
    "    ax.set_ylabel(\"Score (higher is better)\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    plt.legend(title=\"Models\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../../data/outputs/f1_bar.png\", dpi=300)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_levenshtein(df):\n",
    "    fields = [\"title\", \"doi\", \"publication_date\", \"publisher\"]\n",
    "    models = [\"grobid_dl\", \"gpt_oss\", \"phi4_mini\", \"qwen3b\", \"llama3b\"]\n",
    "\n",
    "    melted = df[df[\"metric\"] == \"Levenshtein Dist\"].melt(\n",
    "        id_vars=[\"fields\", \"metric\"], \n",
    "        value_vars=models, \n",
    "        var_name=\"model\", \n",
    "        value_name=\"score\"\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(\n",
    "        data=melted, \n",
    "        x=\"fields\", \n",
    "        y=\"score\", \n",
    "        hue=\"model\"\n",
    "    )\n",
    "    ax.set_title(\"Average Score (Levenshtein Distance) for different Models\")\n",
    "    ax.set_ylabel(\"Score (lower is better)\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    plt.legend(title=\"Models\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../../data/outputs/levenshtein_bar.png\", dpi=300)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_levenshtein(big_df)\n",
    "plot_cosine_and_f1(big_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout-Aware-Metadata-Extraction-Framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
