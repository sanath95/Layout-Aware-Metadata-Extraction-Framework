{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from json import loads, dump, dumps, JSONDecodeError\n",
    "from pydantic import BaseModel, Field, ConfigDict, ValidationError\n",
    "from typing import List\n",
    "import psutil, os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"../../data/metadata_extraction_data/demo/\"\n",
    "\n",
    "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
    "output_dir = Path(\"../../data/metadata_extraction_data/phi4mini_demo_metadata\")\n",
    "response_start_token = \"<|assistant|>\"\n",
    "response_end_token = \"<|end|>\"\n",
    "\n",
    "# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# output_dir = Path(\"../../data/metadata_extraction_data/qwen3b_demo_metadata\")\n",
    "# response_start_token = \"<|im_start|>assistant\\n\"\n",
    "# response_end_token = \"<|im_end|>\"\n",
    "\n",
    "# model_id = \"Qwen/Qwen3-4B-Base\"\n",
    "# output_dir = Path(\"../../data/metadata_extraction_data/qwen4b_demo_metadata\")\n",
    "# response_start_token = \"<|im_start|>assistant\\n\"\n",
    "# response_end_token = \"<|endoftext|>\"\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# output_dir = Path(\"../../data/metadata_extraction_data/llama3b_demo_metadata\")\n",
    "# response_start_token = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "# response_end_token = \"<|eot_id|>\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractMetadata(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured metadata for an academic publication.\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    title: str = Field(\n",
    "        ...,\n",
    "        description=\"The full name identifying the academic publication.\",\n",
    "    )\n",
    "    authors: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"The names of individuals who wrote the publication.\",\n",
    "    )\n",
    "    affiliations: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Institutions or organizations associated with the authors.\",\n",
    "    )\n",
    "    email_ids: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Contact email IDs of the authors.\",\n",
    "    )\n",
    "    publication_date: str = Field(\n",
    "        ...,\n",
    "        description=\"The date when the publication was officially published in DD-MM-YYYY or MM-YYYY or YYYY format.\",\n",
    "    )\n",
    "    publisher: str = Field(\n",
    "        ...,\n",
    "        description=\"The organization responsible for publishing the document.\",\n",
    "    )\n",
    "    doi: str = Field(\n",
    "        ...,\n",
    "        description=\"A unique digital object identifier linking directly to the publication online.\",\n",
    "    )\n",
    "    keywords: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Specific terms highlighting the main topics of the publication.\",\n",
    "    )\n",
    "    abstract: str = Field(\n",
    "        ...,\n",
    "        description=\"A brief summary outlining the publication’s content, methods, and findings.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(file, no_pgs=1):\n",
    "    doc = pymupdf.open(file)\n",
    "    pdf_text = \"\"\n",
    "    for page in doc.pages(0, no_pgs, 1):\n",
    "        pdf_text += page.get_text()+\"\\n\"\n",
    "\n",
    "    return pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = dumps(ExtractMetadata.model_json_schema(), indent=4)\n",
    "system_prompt = (\n",
    "    \"You are given the text from an academic publication. Your task is to extract metadata from the given text.\"\n",
    "    \"The metadata fields you should extract are: Title, Authors, Affiliations, Email IDs, DOI, Publisher, Publication Date, Keywords, and Abstract.\"\n",
    "    \"If any metadata information is missing, leave it blank.\"\n",
    "    f\"Return a JSON with this schema:\\n{response_schema}\\n\"\n",
    "    \"Do not add any preamble or explanations.\"\n",
    "    )\n",
    "user_prompt_template = (\n",
    "    \"## Text:\\n{text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = glob(pdf_dir+\"*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok  = AutoTokenizer.from_pretrained(model_id, cache_dir=\"../../.hf_cache\")\n",
    "bnb_conf = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16) # NOTE: if qwen models dont give proper structured response, try not using \"bnb_conf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16, cache_dir=\"../../.hf_cache\", trust_remote_code=False, device_map=\"auto\", quantization_config=bnb_conf)\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "t0 = time.perf_counter()\n",
    "tt=[]\n",
    "\n",
    "for file in pdf_files:\n",
    "    count = 0\n",
    "    text_file_path = Path(f\"{output_dir / Path(file).stem}.json\")\n",
    "    if not text_file_path.is_file():\n",
    "        user_prompt = user_prompt_template.format(text=get_pdf_text(file, 1))\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}]\n",
    "        tok.pad_token = tok.eos_token\n",
    "        inputs = tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        attention_mask = torch.ones_like(inputs)\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end   = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.inference_mode():\n",
    "            generated = model.generate(\n",
    "                inputs,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=1024,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                pad_token_id=tok.eos_token_id,\n",
    "            )\n",
    "        end.record();  torch.cuda.synchronize()\n",
    "        seq = generated.sequences[0]\n",
    "        #text = tok.batch_decode(generated)[0]\n",
    "        text = tok.decode(seq, skip_special_tokens=False)\n",
    "        clean_text = text.split(response_start_token)[1].split(response_end_token)[0].replace(\"\\n\", \"\").replace(\"```json\",\"\").replace(\"```\",\"\")\n",
    "        try:\n",
    "            op = loads(clean_text)\n",
    "        except JSONDecodeError as e:\n",
    "            op = {\n",
    "                \"title\": \"\",\n",
    "                \"authors\": [],\n",
    "                \"affiliations\": [],\n",
    "                \"email_ids\": [],\n",
    "                \"doi\": \"\",\n",
    "                \"publisher\": \"\",\n",
    "                \"publication_date\": \"\",\n",
    "                \"keywords\": [],\n",
    "                \"abstract\": \"\"\n",
    "            }\n",
    "            print(file, clean_text[-50:], e)\n",
    "        tt.append(start.elapsed_time(end))\n",
    "\n",
    "        try:\n",
    "            metadata = ExtractMetadata(**op)\n",
    "        except ValidationError as e:\n",
    "            print(\"ERROR: \", file, e)\n",
    "\n",
    "        with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "            dump(op, f, ensure_ascii=False, indent=4)\n",
    "        print(\"Wrote file to: \", text_file_path)\n",
    "    else:\n",
    "        print(\"Metadata exists\")\n",
    "torch.cuda.synchronize()          # wait for the GPU\n",
    "elapsed_s   = time.perf_counter() - t0\n",
    "gpu_alloc   = torch.cuda.max_memory_allocated()  / 1024**2   # MiB actually used\n",
    "gpu_reserved= torch.cuda.max_memory_reserved()   / 1024**2   # MiB reserved by the allocator\n",
    "cpu_mem     = process.memory_info().rss          / 1024**2   # MiB\n",
    "\n",
    "print(f\"elapsed {elapsed_s:,.2f}s | GPU used {gpu_alloc:.0f} MiB | \"\n",
    "      f\"GPU reserved {gpu_reserved:.0f} MiB | CPU {cpu_mem:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-oss using ollama\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "model_id = 'gpt-oss:20b'\n",
    "output_dir = Path(\"../../data/metadata_extraction_data/gpt_oss_demo_metadata\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tt = []\n",
    "failed=False\n",
    "for file in pdf_files:\n",
    "    text_file_path = Path(f\"{output_dir / Path(file).stem}.json\")\n",
    "\n",
    "    if not text_file_path.is_file():\n",
    "        user_prompt = user_prompt_template.format(text=get_pdf_text(file, 2))\n",
    "        start = time.time()\n",
    "        messages=[\n",
    "                {\n",
    "                    'role': 'system',\n",
    "                    'content': system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': user_prompt,\n",
    "                }\n",
    "            ]\n",
    "        response: ChatResponse = chat(model=model_id, messages=messages)\n",
    "        time_taken = time.time() - start\n",
    "        tt.append(round(time_taken,2))\n",
    "        text = response['message']['content']\n",
    "\n",
    "        try:\n",
    "            op = loads(text)\n",
    "        except JSONDecodeError as e:\n",
    "            print(file, text[-50:])\n",
    "            op = {\n",
    "                \"title\": \"\",\n",
    "                \"authors\": [],\n",
    "                \"affiliations\": [],\n",
    "                \"email_ids\": [],\n",
    "                \"doi\": \"\",\n",
    "                \"publisher\": \"\",\n",
    "                \"publication_date\": \"\",\n",
    "                \"keywords\": [],\n",
    "                \"abstract\": \"\"\n",
    "            }\n",
    "            failed=True\n",
    "        try:\n",
    "            metadata = ExtractMetadata(**op)\n",
    "        except ValidationError as e:\n",
    "            print(\"ERROR: \", file, e)\n",
    "        if not failed:\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                dump(op, f, ensure_ascii=False, indent=4)\n",
    "            print(\"Wrote file to: \", text_file_path)\n",
    "    else:\n",
    "        print(\"Metadata exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout-Aware-Metadata-Extraction-Framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
