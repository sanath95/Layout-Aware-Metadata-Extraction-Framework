{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from jiwer import wer, cer, process_words\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "import pymupdf\n",
    "import pypdfium2 as pdfium\n",
    "from pdfminer.high_level import extract_text\n",
    "from PyPDF2 import PdfReader\n",
    "import unicodedata\n",
    "from rouge_score import rouge_scorer\n",
    "from scipy.stats import kendalltau\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_text_from_pymupdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract plain text from the first page of the PDF.\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    for page in doc.pages(0, 1, 1):\n",
    "        text_pymupdf = page.get_text()\n",
    "    text_pymupdf = text_pymupdf.replace(\"\\n\", \" \")\n",
    "    return text_pymupdf\n",
    "\n",
    "def extract_text_from_pypdfium2(pdf_path: str) -> str:\n",
    "    \"\"\"Extract plain text from the first page of the PDF.\"\"\"\n",
    "    pdf = pdfium.PdfDocument(pdf_path)\n",
    "    page = pdf[0]\n",
    "    textpage = page.get_textpage()\n",
    "\n",
    "    # Extract text from the whole page\n",
    "    text_pypdfium = textpage.get_text_bounded()\n",
    "\n",
    "    text_pypdfium = text_pypdfium.replace(\"\\n\", \" \")\n",
    "    return text_pypdfium\n",
    "\n",
    "def extract_text_from_pdfminer(pdf_path: str) -> str:\n",
    "    \"\"\"Extract plain text from the first page of the PDF.\"\"\"\n",
    "    text_pdfminer = extract_text(pdf_path, page_numbers=[0])#, laparams=lap)\n",
    "    text_pdfminer = text_pdfminer.replace(\"\\n\", \" \")\n",
    "    return text_pdfminer\n",
    "\n",
    "def extract_text_from_pypdf2(pdf_path: str) -> str:\n",
    "    \"\"\"Extract plain text from the first page of the PDF.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    page = reader.pages[0]\n",
    "    text_pypdf=page.extract_text()\n",
    "    text_pypdf = text_pypdf.replace(\"\\n\", \" \")\n",
    "    return text_pypdf\n",
    "\n",
    "def alto_to_text(alto_xml_path, merge_hyphens=True):\n",
    "    \"\"\"\n",
    "    Extract plain text from an ALTO XML (pdfalto output).\n",
    "    - Preserves page and line breaks.\n",
    "    - Optionally merges line-end hyphenations.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(alto_xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Common ALTO namespaces (some files include ns)\n",
    "    ns = {}\n",
    "    if root.tag.startswith(\"{\"):\n",
    "        ns_uri = root.tag.split(\"}\")[0].strip(\"{\")\n",
    "        ns = {\"a\": ns_uri}\n",
    "\n",
    "    # Helper to find with/without namespace\n",
    "    def findall(elem, path):\n",
    "        return elem.findall(path if ns else path.replace(\"a:\", \"\" ), ns)\n",
    "\n",
    "    pages = findall(root, \".//a:Page\" if ns else \".//Page\")\n",
    "    out_lines = []\n",
    "\n",
    "    for _, page in enumerate(pages[:1], start=1):\n",
    "        # Text blocks -> text lines -> strings\n",
    "        blocks = findall(page, \".//a:TextBlock\" if ns else \".//TextBlock\")\n",
    "        page_lines = []\n",
    "        for block in blocks:\n",
    "            lines = findall(block, \".//a:TextLine\" if ns else \".//TextLine\")\n",
    "            for line in lines:\n",
    "                tokens = []\n",
    "                # <String CONTENT=\"...\"> are words\n",
    "                for s in findall(line, \".//a:String\" if ns else \".//String\"):\n",
    "                    tok = s.attrib.get(\"CONTENT\", \"\")\n",
    "                    if tok:\n",
    "                        tokens.append(tok)\n",
    "                # Optional <SP> nodes mean explicit spaces in some ALTOs; tokens already spaced below.\n",
    "                line_text = \" \".join(tokens)\n",
    "\n",
    "                # Merge hyphenation across line breaks: line ending with '-' and next line starts with lowercase/alpha\n",
    "                if merge_hyphens and page_lines:\n",
    "                    prev = page_lines[-1]\n",
    "                    if prev.endswith(\"-\") and line_text[:1].isalnum():\n",
    "                        page_lines[-1] = prev[:-1] + line_text.lstrip()\n",
    "                        continue\n",
    "\n",
    "                page_lines.append(line_text)\n",
    "\n",
    "        # Add page text and a form-feed separator\n",
    "        out_lines.append(\"\\n\".join(page_lines))\n",
    "        out_lines.append(\"\\f\")  # form feed between pages\n",
    "\n",
    "    text = \"\\n\".join(out_lines).rstrip(\"\\f\\n\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def calculate_kendall_tau(ref, hyp):\n",
    "    # 3) reading-order (Kendall τ over shared tokens)\n",
    "    ref_tokens = ref.split()\n",
    "    hyp_tokens = hyp.split()\n",
    "    positions, ref_map = [], {}\n",
    "    for i, tok in enumerate(ref_tokens):\n",
    "        ref_map.setdefault(tok, []).append(i)\n",
    "    for tok in hyp_tokens:\n",
    "        if tok in ref_map and ref_map[tok]:\n",
    "            positions.append(ref_map[tok].pop(0))\n",
    "    tau, _ = kendalltau(positions, sorted(positions)) if len(positions) > 1 else (1, None)\n",
    "    order_error = (1 - tau)                              # 0 = perfect order, 2 = reversed\n",
    "    return order_error\n",
    "\n",
    "def _strip_controls(text: str) -> str:\n",
    "    \"\"\"Drop all Unicode control characters (category C*).\"\"\"\n",
    "    return \"\".join(c for c in text if unicodedata.category(c)[0] != \"C\")\n",
    "\n",
    "def _fix_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove diacritics (accents) but keep other characters unchanged.\n",
    "    \"\"\"\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def _clean_item(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lower-case, strip accents, trim whitespace, collapse internal spaces.\n",
    "    \"\"\"\n",
    "    text = _strip_controls(_fix_accents(text.lower()))\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_alignment_metrics(ref_text: str, hyp_text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Align reference and hypothesis texts at word level, returning:\n",
    "    (hits, substitutions, deletions, insertions).\n",
    "    \"\"\"\n",
    "    # Use jiwer to get alignment info\n",
    "    alignment = process_words([ref_text], [hyp_text])\n",
    "    hits = alignment.hits         # correctly matched words\n",
    "    subs = alignment.substitutions  # substitutions (words in ref replaced by others in hyp)\n",
    "    dels = alignment.deletions     # deletions (words missing in hyp)\n",
    "    ins = alignment.insertions     # insertions (extra words in hyp)\n",
    "    return hits, subs, dels, ins\n",
    "\n",
    "def compute_metrics(ref_text: str, hyp_text: str, arxiv_id) -> dict:\n",
    "    \"\"\"Compute WER, CER, Precision, Recall, F1, and BLEU given reference and hypothesis text.\"\"\"\n",
    "    metrics = {}\n",
    "    # Word Error Rate and Character Error Rate\n",
    "    metrics[\"WER\"] = round(wer(ref_text, hyp_text), 4)            # e.g., 0.05 for 5% error\n",
    "    metrics[\"CER\"] = round(cer(ref_text, hyp_text), 4)            # e.g., 0.02 for 2% char error\n",
    "\n",
    "    # Alignment-based metrics for Precision, Recall, F1\n",
    "    hits, subs, dels, ins = compute_alignment_metrics(ref_text, hyp_text)\n",
    "    # Avoid division by zero\n",
    "    total_pred_words = hits + subs + ins  # total words in hypothesis\n",
    "    total_ref_words = hits + subs + dels  # total words in reference\n",
    "    if total_pred_words == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = hits / total_pred_words\n",
    "    if total_ref_words == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = hits / total_ref_words\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    metrics[\"precision\"] = round(precision, 4)\n",
    "    metrics[\"recall\"] = round(recall, 4)\n",
    "    metrics[\"f1\"] = round(f1, 4)\n",
    "\n",
    "    metrics[\"order_error\"] = round(calculate_kendall_tau(ref_text, hyp_text), 4)\n",
    "\n",
    "    # BLEU score using sacrebleu\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses=[hyp_text], references=[[ref_text]])\n",
    "    metrics[\"BLEU\"] = round(bleu.score / 100.0, 4)  # sacrebleu.score is out of 100, convert to 0-1\n",
    "    metrics[\"rouge_score\"] = round(rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\\\n",
    "                          .score(target=ref_text, prediction=hyp_text)['rougeL'].fmeasure, 4)\n",
    "    metrics[\"arxiv_id\"] = arxiv_id\n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    scores =  {}\n",
    "    tools = [\"pymupdf\", \"pypdfium2\", \"pdfminer\", \"pypdf2\", \"pdfalto\"]\n",
    "    for t in tools: scores[t] = []\n",
    "\n",
    "    df = pd.read_csv(\"../../data/pdf_extraction_data/metadata.csv\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        pdf_file = row.loc[\"pdf_path\"]\n",
    "        annotation = row.loc[\"text\"]\n",
    "        arxiv_id = row.loc[\"arxiv_id\"]\n",
    "\n",
    "\n",
    "        # Extract text from PDF first page\n",
    "        text_pymupdf = _clean_item(extract_text_from_pymupdf(pdf_file))\n",
    "        text_pypdfium = _clean_item(extract_text_from_pypdfium2(pdf_file))\n",
    "        text_pdfminer = _clean_item(extract_text_from_pdfminer(pdf_file))\n",
    "        text_pypdf = _clean_item(extract_text_from_pypdf2(pdf_file))\n",
    "        text_pdfalto = _clean_item(alto_to_text(\"../../data/pdf_extraction_data/pdfalto_xml/\" + Path(pdf_file).stem + \".xml\"))\n",
    "        # Read the ground truth text from file\n",
    "        with open(annotation, 'r', encoding='utf-8') as f:\n",
    "            ground_truth_text = f.read()\n",
    "            \n",
    "        # Normalize both texts\n",
    "        norm_truth = _clean_item(ground_truth_text)\n",
    "\n",
    "        # Compute all metrics\n",
    "        #results = compute_metrics(norm_truth, text_pymupdf)\n",
    "        #print(results)\n",
    "        scores[\"pymupdf\"].append(compute_metrics(norm_truth, text_pymupdf, arxiv_id))\n",
    "        scores[\"pypdfium2\"].append(compute_metrics(norm_truth, text_pypdfium, arxiv_id))\n",
    "        scores[\"pdfminer\"].append(compute_metrics(norm_truth, text_pdfminer, arxiv_id))\n",
    "        scores[\"pypdf2\"].append(compute_metrics(norm_truth, text_pypdf, arxiv_id))\n",
    "        scores[\"pdfalto\"].append(compute_metrics(norm_truth, text_pdfalto, arxiv_id))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../../data/outputs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   # convenient for grouped box-plots\n",
    "\n",
    "\n",
    "# ---- reshape to “long” format ----\n",
    "records = [\n",
    "    {\"Tool\": tool, **metric_dict}\n",
    "    for tool, metric_list in s.items()\n",
    "    for metric_dict in metric_list\n",
    "]\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Melt so each metric becomes a row instead of a column\n",
    "long_df = df.melt(\n",
    "    id_vars=\"Tool\",\n",
    "    value_vars=[\"CER\", \"WER\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Score\",\n",
    ")\n",
    "\n",
    "# ---- draw the box-plot ----\n",
    "sns.set(style=\"whitegrid\")                    # nicer defaults\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(\n",
    "    data=long_df,\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Tool\",\n",
    "    width=0.7,\n",
    ")\n",
    "ax.set_title(\"Evaluating PDF Parsers\")\n",
    "ax.set_ylabel(\"Error rate (%)\")\n",
    "ax.set_xlabel(\"\")\n",
    "plt.legend(title=\"Tool\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig(\"../../data/outputs/error_rates.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   # convenient for grouped box-plots\n",
    "\n",
    "\n",
    "# ---- reshape to “long” format ----\n",
    "records = [\n",
    "    {\"Tool\": tool, **metric_dict}\n",
    "    for tool, metric_list in s.items()\n",
    "    for metric_dict in metric_list\n",
    "]\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Melt so each metric becomes a row instead of a column\n",
    "long_df = df.melt(\n",
    "    id_vars=\"Tool\",\n",
    "    value_vars=[\"BLEU\", \"rouge_score\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Score\",\n",
    ")\n",
    "\n",
    "# ---- draw the box-plot ----\n",
    "sns.set(style=\"whitegrid\")                    # nicer defaults\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(\n",
    "    data=long_df,\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Tool\",\n",
    "    width=0.7,\n",
    ")\n",
    "ax.set_title(\"Evaluating PDF Parsers\")\n",
    "ax.set_ylabel(\"Order Measures\")\n",
    "ax.set_xlabel(\"\")\n",
    "plt.legend(title=\"Tool\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig(\"../../data/outputs/order_measures.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from numbers import Number   # to detect numeric values\n",
    "\n",
    "def summarize_metrics(data, verbose=True):\n",
    "    \"\"\"\n",
    "    Compute average and max for every numeric metric inside each extractor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict[str, list[dict]]\n",
    "        Outer keys are extractor names; values are lists of metric dictionaries.\n",
    "    verbose : bool, default True\n",
    "        If True, print a neat table; the function always returns the summary dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, float]]\n",
    "        Nested dictionary: {extractor: {metric: value, ...}, ...}\n",
    "        For each metric both 'avg_<metric>' and 'max_<metric>' are provided.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "\n",
    "    for extractor, records in data.items():\n",
    "        # Collect metric → list of values\n",
    "        metric_values = defaultdict(list)\n",
    "\n",
    "        for rec in records:\n",
    "            for metric, value in rec.items():\n",
    "                if isinstance(value, Number):\n",
    "                    metric_values[metric].append(value)\n",
    "\n",
    "        # Build the statistics\n",
    "        stats = {}\n",
    "        for metric, values in metric_values.items():\n",
    "            stats[f'avg_{metric}'] = mean(values)\n",
    "            #stats[f'max_{metric}'] = max(values)\n",
    "\n",
    "        summary[extractor] = stats\n",
    "\n",
    "    if verbose:\n",
    "        # Pretty-print: each extractor on its own line\n",
    "        for extractor, stats in summary.items():\n",
    "            parts = [f\"{k}={v:.4f}\" for k, v in sorted(stats.items())]\n",
    "            print(f\"{extractor:<10}  \" + \"  \".join(parts))\n",
    "\n",
    "    return summary\n",
    "\n",
    "x = summarize_metrics(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "from math import inf\n",
    "\n",
    "# set of metrics for which we want the minimum\n",
    "min_metrics = {'avg_WER', 'avg_CER', 'avg_order_error'}\n",
    "\n",
    "# discover all avg_ metrics\n",
    "all_metrics = {m for stats in x.values() for m in stats if m.startswith(\"avg_\")}\n",
    "\n",
    "# find the best tool for each metric, using min or max as specified\n",
    "best_locations = {}\n",
    "for metric in all_metrics:\n",
    "    if metric in min_metrics:\n",
    "        # for these, pick the tool with the smallest value\n",
    "        best_tool, best_val = min(\n",
    "            x.items(),\n",
    "            key=lambda kv: kv[1].get(metric, inf)\n",
    "        )\n",
    "    else:\n",
    "        # for all others, pick the tool with the largest value\n",
    "        best_tool, best_val = max(\n",
    "            x.items(),\n",
    "            key=lambda kv: kv[1].get(metric, -inf)\n",
    "        )\n",
    "\n",
    "    # strip off \"avg_\" prefix if you like\n",
    "    clean_name = metric[len(\"avg_\"):]\n",
    "    best_locations[clean_name] = best_tool\n",
    "\n",
    "best_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout-Aware-Metadata-Extraction-Framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
