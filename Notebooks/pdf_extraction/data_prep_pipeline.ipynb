{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "from ultralytics import YOLO\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from re import split\n",
    "from collections import defaultdict\n",
    "from numpy import array, asarray\n",
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "import sys\n",
    "sys.path.append('../../layoutreader')\n",
    "from v3.helpers import boxes2inputs, prepare_inputs, parse_logits\n",
    "from torch import no_grad\n",
    "import pandas as pd\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../../data/pdf_extraction_data\")\n",
    "\n",
    "annotation_dir = Path(output_dir) / Path(\"annotation\")\n",
    "\n",
    "text_dir = Path(output_dir) / Path(\"txt\")\n",
    "text_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "img_dir = Path(output_dir) / Path(\"img\")\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata_path = Path(output_dir) / Path(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_words(regions, words):\n",
    "    # Assign IDs and compute area for regions\n",
    "    for idx, region in enumerate(regions):\n",
    "        region['id'] = idx\n",
    "        x0, y0, x1, y1 = region['bbox']\n",
    "        region['area'] = (x1 - x0) * (y1 - y0)\n",
    "    \n",
    "    # Sort regions by area (smallest first)\n",
    "    regions_sorted = sorted(regions, key=lambda r: r['area'])\n",
    "    \n",
    "    grouped_words = defaultdict(list)\n",
    "    #grouped_words_text = defaultdict(list)\n",
    "    unassigned = []\n",
    "    \n",
    "    # Assign words to regions\n",
    "    for word in words:\n",
    "        x0, y0, x1, y1 = word[0:4]\n",
    "        center_x, center_y = (x0 + x1) / 2, (y0 + y1) / 2\n",
    "        assigned = False\n",
    "        \n",
    "        for region in regions_sorted:\n",
    "            r_x0, r_y0, r_x1, r_y1 = region['bbox']\n",
    "            if r_x0-10 <= center_x <= r_x1+10 and r_y0-10 <= center_y <= r_y1+10:\n",
    "                grouped_words[region['id']].append(word)\n",
    "                assigned = True\n",
    "                break\n",
    "                \n",
    "        if not assigned:\n",
    "            unassigned.append(word)\n",
    "    \n",
    "    # Prepare result (sort words in reading order)\n",
    "    result = []\n",
    "    for region in regions:\n",
    "        words_in_region = grouped_words.get(region['id'], [])\n",
    "        words_sorted = sorted(words_in_region, key=lambda w: (w[1]))  # Sort by y0, then x0\n",
    "        words_sorted = sort_by_line(words_sorted)\n",
    "        result.append({\n",
    "            \"region_id\": region['id'],\n",
    "            \"label\": region['label'],\n",
    "            \"bbox\": region['bbox'],\n",
    "            \"words\": words_sorted\n",
    "        })\n",
    "    \n",
    "    return result, unassigned\n",
    "\n",
    "def sort_by_line(data):\n",
    "    buckets = defaultdict(list)\n",
    "    \n",
    "    for item in data:\n",
    "        _, y0, *_ = item[:4]\n",
    "        bucket = int(y0) // 10          # integer division → 10-pixel band\n",
    "        buckets[bucket].append(item)\n",
    "\n",
    "    # Produce a list of groups ordered top-to-bottom (smallest y0 first)\n",
    "    grouped_lines = []\n",
    "    for bucket in sorted(buckets):                     # top-to-bottom\n",
    "        line = sorted(buckets[bucket], key=lambda t: t[0])   # left-to-right\n",
    "        grouped_lines.append(line)\n",
    "\n",
    "    flat = [item for sublist in grouped_lines for item in sublist]\n",
    "    return flat\n",
    "\n",
    "def rescale_bboxes(bboxes_xyxy,                  # (N,4) or (4,)  -> x0,y0,x1,y1\n",
    "                   src_size,\n",
    "                   dst_size):\n",
    "    \"\"\"\n",
    "    Linearly rescales axis-aligned boxes from one pixel/grid space to another.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bboxes_xyxy : array-like\n",
    "        Box coordinates in the *source* space.\n",
    "    src_size    : (w, h) tuple\n",
    "    dst_size    : (w, h) tuple\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray  of shape (..., 4) in the destination space.\n",
    "    \"\"\"\n",
    "    b = asarray(bboxes_xyxy, dtype=float)\n",
    "    sx = dst_size[0] / src_size[0]\n",
    "    sy = dst_size[1] / src_size[1]\n",
    "    scale = array([sx, sy, sx, sy])\n",
    "    return b * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = hf_hub_download(\n",
    "        repo_id=\"hantian/yolo-doclaynet\",\n",
    "        filename=\"yolov12s-doclaynet.pt\",\n",
    "        cache_dir=\"../../.hf_cache\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(metadata_path)\n",
    "new_pdf = False\n",
    "total_unassigned = 0\n",
    "total_tokens = 0\n",
    "for i, row in df.iterrows():\n",
    "    if not row[\"text\"]:\n",
    "        annotation_file = row[\"annotation\"]\n",
    "        pdf_file = row[\"pdf_path\"]\n",
    "        arxiv_id = row['arxiv_id']\n",
    "        \n",
    "        doc = pymupdf.open(pdf_file)\n",
    "        for page in doc.pages(0, 1, 1):\n",
    "            w,h = page.mediabox[-2], page.mediabox[-1]\n",
    "\n",
    "        pages_as_image = convert_from_path(pdf_file, dpi=300)\n",
    "        model   = YOLO(weight_path)\n",
    "        preds = model(pages_as_image[0])[0]\n",
    "\n",
    "        regions = []\n",
    "        labels = preds.names\n",
    "        for xyxy, cls, score in zip(preds.boxes.xyxy, preds.boxes.cls, preds.boxes.conf):\n",
    "            label = labels[int(cls)]\n",
    "            bbox = xyxy.tolist()\n",
    "            regions.append({\"label\": label, \"bbox\": bbox})\n",
    "\n",
    "        annotated = preds.plot()\n",
    "        Image.fromarray(annotated[..., ::-1]).save(img_dir / f\"{Path(pdf_file).stem}.png\")\n",
    "\n",
    "        annotation = annotation_dir / Path(annotation_file)      # one page\n",
    "        boxes = []\n",
    "\n",
    "        for line in annotation.read_text(encoding=\"utf-8\").splitlines():\n",
    "            parts = split(r\"\\s+\", line.rstrip())\n",
    "            if parts[0] != \"##LTLine##\":\n",
    "                x0, y0, x1, y1 = map(int, parts[1:5])  # already 0-1000\n",
    "                boxes.append([x0, y0, x1, y1, parts[0]])\n",
    "                total_tokens+=1\n",
    "\n",
    "        for r in regions:\n",
    "            r[\"bbox\"] = rescale_bboxes(r[\"bbox\"], preds.orig_shape, (h, w))\n",
    "\n",
    "        norm = lambda v, maxv: int(1000 * v / maxv)\n",
    "        for r in regions:\n",
    "            r[\"bbox\"] = [norm(r[\"bbox\"][0],w), norm(r[\"bbox\"][1],h), norm(r[\"bbox\"][2],w), norm(r[\"bbox\"][3],h)]\n",
    "\n",
    "        grouped_data, unassigned = group_words(regions, boxes)\n",
    "\n",
    "        boxes = []\n",
    "        tokens = []\n",
    "        for g in grouped_data:\n",
    "            if g[\"words\"]:\n",
    "                boxes.append((min(item[0] for item in g[\"words\"]), min(item[1] for item in g[\"words\"]), max(item[2] for item in g[\"words\"]), max(item[3] for item in g[\"words\"])))\n",
    "                tokens.append(\" \".join([item[4] for item in g[\"words\"]]))\n",
    "        if unassigned:\n",
    "            for u in unassigned:\n",
    "                boxes.append(tuple(u[:4]))\n",
    "                tokens.append(u[4])\n",
    "                total_unassigned+=1\n",
    "        model  = LayoutLMv3ForTokenClassification.from_pretrained(\"hantian/layoutreader\").to(\"cuda\")\n",
    "        inputs = boxes2inputs(boxes)\n",
    "        inputs = prepare_inputs(inputs, model)     # pads/truncates for the model\n",
    "\n",
    "        with no_grad():\n",
    "            logits = model(**inputs).logits.squeeze(0)   # (n_boxes, n_classes)\n",
    "\n",
    "        order = parse_logits(logits, len(boxes))         # e.g. [3,0,1,2,…]\n",
    "\n",
    "        # rearrange text (or any metadata) with the predicted order\n",
    "        y = \" \".join(tokens[i].strip() for i in order)\n",
    "\n",
    "        text_file_path = text_dir / Path(f\"{arxiv_id}.txt\")\n",
    "        with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(y)\n",
    "\n",
    "        df.loc[df[\"arxiv_id\"]==arxiv_id, \"text\"] = str(text_file_path)\n",
    "\n",
    "        new_pdf = True\n",
    "\n",
    "if new_pdf: \n",
    "    df.to_csv(metadata_path, index=False)\n",
    "    if total_tokens>0: print(f\"{round(total_unassigned/total_tokens * 100, 2)}% tokens unassigned\")\n",
    "else: print('Files already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Layout-Aware-Metadata-Extraction-Framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
