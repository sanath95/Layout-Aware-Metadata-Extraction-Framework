{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from itertools import combinations\n",
    "from deepdiff import DeepDiff\n",
    "import pymupdf\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "import yaml\n",
    "from time import sleep, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractMetadata(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured metadata for an academic publication.\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    title: str = Field(\n",
    "        ...,\n",
    "        description=\"The full name identifying the academic publication.\",\n",
    "    )\n",
    "    authors: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"The names of individuals who wrote the publication.\",\n",
    "    )\n",
    "    affiliations: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Institutions or organizations associated with the authors.\",\n",
    "    )\n",
    "    email_ids: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Contact email IDs of the authors.\",\n",
    "    )\n",
    "    publication_date: str = Field(\n",
    "        ...,\n",
    "        description=\"The date when the publication was officially published in DD-MM-YYYY format.\",\n",
    "    )\n",
    "    publisher: str = Field(\n",
    "        ...,\n",
    "        description=\"The organization responsible for publishing the document.\",\n",
    "    )\n",
    "    doi: str = Field(\n",
    "        ...,\n",
    "        description=\"A unique digital object identifier linking directly to the publication online.\",\n",
    "    )\n",
    "    keywords: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Specific terms highlighting the main topics of the publication.\",\n",
    "    )\n",
    "    abstract: str = Field(\n",
    "        ...,\n",
    "        description=\"A brief summary outlining the publication’s content, methods, and findings.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "extract_metadata_system_prompt = config[\"prompts\"][\"extract_metadata_system_prompt\"]\n",
    "evaluate_metadata_system_prompt = config[\"prompts\"][\"evaluate_metadata_system_prompt\"]\n",
    "\n",
    "models = config[\"models\"]['extract_metadata']\n",
    "JUDGE_MODEL = config[\"models\"]['evaluate_metadata']\n",
    "\n",
    "METADATA_FIELDS = config[\"metadata_fields\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3daed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = Path(\"../data/metadata_extraction_data/pdf\")\n",
    "\n",
    "output_dir = Path(\"../data/metadata_extraction_data/metadata\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eabaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyMuPDF. If max_pages is set, limit to that many pages.\"\"\"\n",
    "    with pymupdf.open(pdf_path) as doc:\n",
    "        text = \"\\n\".join([page.get_text() for page in doc.pages()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c007e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(model, system_prompt, user_prompt, name, response_schema):\n",
    "    \"\"\"Send a chat completion request to the specified model via OpenRouter and return the assistant's response.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"temperature\": 0,\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": name,\n",
    "                \"strict\": True,\n",
    "                \"schema\": response_schema\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    sleep(10)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2876f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_level_agreement(model_dicts):\n",
    "    agreed = {}\n",
    "    disagreements = {m: {} for m in model_dicts.keys()}\n",
    "    \n",
    "    for field in METADATA_FIELDS:\n",
    "        \n",
    "        values = {m: (d or {}).get(field, \"\") for m, d in model_dicts.items()}\n",
    "        \n",
    "        names = list(values.keys())\n",
    "        \n",
    "        pairs = list(combinations(names, 2))\n",
    "        agreed_value = None\n",
    "        agreed_count = 0\n",
    "        for m_x, m_y in pairs:\n",
    "            diff = DeepDiff(values[m_x], values[m_y], ignore_order=True)\n",
    "            if not diff:\n",
    "                agreed_value = values[m_x]\n",
    "                agreed_count+=1\n",
    "        if agreed_value is not None and agreed_count==len(model_dicts.keys()):\n",
    "            agreed[field] = agreed_value\n",
    "        else:\n",
    "            for m, val in values.items():\n",
    "                disagreements[m][field] = val\n",
    "    disagreements = {m: v for m, v in disagreements.items() if v}\n",
    "    return agreed, disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cf2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_judge_prompt(excerpt_text, disagreements):\n",
    "    \"\"\"\n",
    "    Build the user prompt for the judge:\n",
    "      – include paper excerpt\n",
    "      – show only the *disputed* fields from each model\n",
    "    \"\"\"\n",
    "    prompt_parts = [f\"Paper Text (Excerpt):\\n{excerpt_text}\\n\"]\n",
    "    for model_name, fields in disagreements.items():\n",
    "        prompt_parts.append(f\"Disputed fields from {model_name}:\\n{json.dumps(fields, indent=2)}\")\n",
    "    return \"\\n\".join(prompt_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = glob(\"../data/metadata_extraction_data/pdf/*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata = {}\n",
    "for pdf_path in pdf_files:\n",
    "    text_file_path = Path(f\"{output_dir / Path(pdf_path).stem}.json\")\n",
    "    if not text_file_path.is_file():\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        model_outputs = {}\n",
    "        for model in models:\n",
    "            \n",
    "            raw_response = query_llm(model, extract_metadata_system_prompt, pdf_text, \"ExtractMetadata\", ExtractMetadata.model_json_schema())\n",
    "            \n",
    "            try:\n",
    "                metadata = json.loads(raw_response)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Something went wrong with metadata extraction for {pdf_path} with {model}\")\n",
    "                metadata = None\n",
    "            \n",
    "            model_outputs[model] = metadata if metadata is not None else raw_response\n",
    "\n",
    "        agreed, disagreements = field_level_agreement(model_outputs)\n",
    "\n",
    "        if not disagreements:\n",
    "            final_metadata = agreed\n",
    "            final_analysis  = \"All three models were in complete agreement for every field.\"\n",
    "        else:\n",
    "            judge_user_prompt = build_judge_prompt(pdf_text, disagreements)\n",
    "            res_schema = ExtractMetadata.model_json_schema()\n",
    "            disagreed_fields = [k for k in disagreements[models[0]].keys()]\n",
    "            res_schema[\"required\"] = disagreed_fields\n",
    "            for k in list(res_schema[\"properties\"].keys()):\n",
    "                if not k in disagreed_fields:\n",
    "                    res_schema[\"properties\"].pop(k, None)\n",
    "            fields_str = \", \".join(disagreed_fields)\n",
    "            evaluate_metadata_system_prompt = evaluate_metadata_system_prompt.format(fields=fields_str)\n",
    "            judge_reply = query_llm(JUDGE_MODEL, evaluate_metadata_system_prompt, judge_user_prompt, \"JudgeMetadata\", res_schema)\n",
    "            try:\n",
    "                judged_meta = json.loads(judge_reply)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Something went wrong with metadata judge for {pdf_path}\")\n",
    "                judged_meta = {}\n",
    "            final_metadata = {**agreed, **judged_meta}\n",
    "        all_metadata[pdf_path] = final_metadata\n",
    "        \n",
    "        with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_metadata, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Extracted metadata to: {text_file_path}\")\n",
    "    else:\n",
    "        print(f\"Metadata already exists: {text_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
